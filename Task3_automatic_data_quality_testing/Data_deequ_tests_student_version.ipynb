{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Big Data - Deequ Analysis\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "## Honour Code\n",
    "I {**NELSON**, **MWEMBE**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "    Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "Having completed manual data quality checks, it should be obvious that the process can become quite cumbersome. As the Data Engineer in the team, you have researched some tools that could potentially save the team from having to do this cumbersome work. In your research, you have come a across a tool called [Deequ](https://github.com/awslabs/deequ), which is a library for measuring the data quality of large datasets.\n",
    "\n",
    "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/raw/master/data_engineering/transform/predict/DataQuality.jpg\"\n",
    "     alt=\"Data Quality\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=100%/>\n",
    "     <p><em>Figure 1. Six dimensions of data quality</em></p>\n",
    "</div>\n",
    "\n",
    "You present this tool to your manager; he is quite impressed and gives you the go-ahead to use this in your implementation. You are now required to perform some data quality tests using this automated data testing tool.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 🚩️ Important Notice 🚩️\n",
    ">\n",
    ">To successfully run `pydeequ` without any errors, please make sure that you have an environment that is running pyspark version 3.0.\n",
    "> You are advised to **create a new conda environment** and install this specific version of pyspark to avoid any technical issues:\n",
    ">\n",
    "> `pip install pyspark==3.0`\n",
    "\n",
    "<br>\n",
    "\n",
    "## Import dependencies\n",
    "\n",
    "If you do not have `pydeequ` already installed, install it using the following command:\n",
    "- `pip install pydeequ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pydeequ\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.profiles import *\n",
    "from pydeequ.suggestions import *\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into spark dataframe\n",
    "\n",
    "In this notebook, we set out to run some data quality tests, with the possiblity of running end to end on the years 1963, 1974, 1985, 1996, 2007, and 2018. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Data_ingestion_student_version.ipynb` notebook to create the parquet files for the following years:\n",
    ">       - 1963\n",
    ">       - 1974\n",
    ">       - 1985\n",
    ">       - 1996\n",
    ">       - 2007\n",
    ">       - 2018\n",
    ">\n",
    ">2. Ingest the data for the for the years given above. You should only do it one year at a time.\n",
    ">3. Ingest the metadata file.\n",
    "\n",
    "\n",
    "When developing your code, it will be sufficient to focus on a single year. However, after your development is done, you will need to run this notebook for all of the given years above so that you can answer all the questions given in the Data Testing MCQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here\n",
    "# Use this variable (year) to determine which year your are focusing on\n",
    "#year = 1963\n",
    "df_1963 = spark.read.parquet(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/Task3_automatic_data_quality_testing/1963_output\")\n",
    "\n",
    "#year = 1974\n",
    "df_1974 = spark.read.parquet(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/Task3_automatic_data_quality_testing/1974_output\")\n",
    "\n",
    "#year = 1985\n",
    "df_1985 = spark.read.parquet(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/Task3_automatic_data_quality_testing/1985_output\")\n",
    "\n",
    "#year = 1996\n",
    "df_1996 = spark.read.parquet(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/Task3_automatic_data_quality_testing/1996_output\")\n",
    "\n",
    "#year = 2007\n",
    "df_2007 = spark.read.parquet(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/Task3_automatic_data_quality_testing/2007_output\")\n",
    "\n",
    "#year = 2018\n",
    "df_2018 = spark.read.parquet(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/Task3_automatic_data_quality_testing/2018_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = spark.read.csv(\"C:/Users/nmwem/Downloads/Compressed/processing-big-data-predict-main/symbols_valid_meta.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|stock|\n",
      "+-----+\n",
      "| UTX#|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the incorrect stock ticker included in 1985?\n",
    "parquet_stocks = df_1985.select('stock').distinct()\n",
    "metadata_stocks = metadata.select('Symbol').distinct()\n",
    "inconsistent_stocks = parquet_stocks.exceptAll(metadata_stocks)\n",
    "inconsistent_stocks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------------+----------------+---------------+---+--------------+----------+----------------+----------+-------------+----------+\n",
      "|Nasdaq Traded|Symbol|       Security Name|Listing Exchange|Market Category|ETF|Round Lot Size|Test Issue|Financial Status|CQS Symbol|NASDAQ Symbol|NextShares|\n",
      "+-------------+------+--------------------+----------------+---------------+---+--------------+----------+----------------+----------+-------------+----------+\n",
      "|            Y|   UTX|United Technologi...|               N|               |  N|         100.0|         N|            null|       UTX|          UTX|         N|\n",
      "|            Y| UTX.V|United Technologi...|               N|               |  N|         100.0|         N|            null|      UTXw|         UTX#|         N|\n",
      "+-------------+------+--------------------+----------------+---------------+---+--------------+----------+----------------+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata.createOrReplaceTempView(\"metadata\")\n",
    "\n",
    "spark.sql(\"select * from metadata where Symbol like 'UTX%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run tests on the dataset**\n",
    "\n",
    "## Test 1 - Null values ⛔️\n",
    "For the first test, you are required to check the data for completeness.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check for missing values in the data. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here\n",
    "#year = 1963\n",
    "columns_to_check = df_1963.columns\n",
    "\n",
    "# Define the completeness check for each column\n",
    "checks = [Check(spark, CheckLevel.Warning, f\"Completeness check for column {col}\")\n",
    "          .isComplete(col) for col in columns_to_check]\n",
    "\n",
    "# Run the completeness checks\n",
    "result = VerificationSuite(spark).onData(df_1963)\n",
    "for check in checks:\n",
    "    result = result.addCheck(check)\n",
    "result = result.run()\n",
    "\n",
    "# View the check results\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1974\n",
    "columns_to_check = df_1974.columns\n",
    "\n",
    "# Define the completeness check for each column\n",
    "checks = [Check(spark, CheckLevel.Warning, f\"Completeness check for column {col}\")\n",
    "          .isComplete(col) for col in columns_to_check]\n",
    "\n",
    "# Run the completeness checks\n",
    "result = VerificationSuite(spark).onData(df_1974)\n",
    "for check in checks:\n",
    "    result = result.addCheck(check)\n",
    "result = result.run()\n",
    "\n",
    "# View the check results\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995953...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1985\n",
    "columns_to_check = df_1985.columns\n",
    "\n",
    "# Define the completeness check for each column\n",
    "checks = [Check(spark, CheckLevel.Warning, f\"Completeness check for column {col}\")\n",
    "          .isComplete(col) for col in columns_to_check]\n",
    "\n",
    "# Run the completeness checks\n",
    "result = VerificationSuite(spark).onData(df_1985)\n",
    "for check in checks:\n",
    "    result = result.addCheck(check)\n",
    "result = result.run()\n",
    "\n",
    "# View the check results\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99995461...|\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1996\n",
    "columns_to_check = df_1996.columns\n",
    "\n",
    "# Define the completeness check for each column\n",
    "checks = [Check(spark, CheckLevel.Warning, f\"Completeness check for column {col}\")\n",
    "          .isComplete(col) for col in columns_to_check]\n",
    "\n",
    "# Run the completeness checks\n",
    "result = VerificationSuite(spark).onData(df_1996)\n",
    "for check in checks:\n",
    "    result = result.addCheck(check)\n",
    "result = result.run()\n",
    "\n",
    "# View the check results\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99998847...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2007\n",
    "columns_to_check = df_2007.columns\n",
    "\n",
    "# Define the completeness check for each column\n",
    "checks = [Check(spark, CheckLevel.Warning, f\"Completeness check for column {col}\")\n",
    "          .isComplete(col) for col in columns_to_check]\n",
    "\n",
    "# Run the completeness checks\n",
    "result = VerificationSuite(spark).onData(df_2007)\n",
    "for check in checks:\n",
    "    result = result.addCheck(check)\n",
    "result = result.run()\n",
    "\n",
    "# View the check results\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Completeness chec...|    Warning|     Success|CompletenessConst...|          Success|                    |\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "|Completeness chec...|    Warning|     Warning|CompletenessConst...|          Failure|Value: 0.99996644...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2018\n",
    "columns_to_check = df_2018.columns\n",
    "\n",
    "# Define the completeness check for each column\n",
    "checks = [Check(spark, CheckLevel.Warning, f\"Completeness check for column {col}\")\n",
    "          .isComplete(col) for col in columns_to_check]\n",
    "\n",
    "# Run the completeness checks\n",
    "result = VerificationSuite(spark).onData(df_2018)\n",
    "for check in checks:\n",
    "    result = result.addCheck(check)\n",
    "result = result.run()\n",
    "\n",
    "# View the check results\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Zero Values 🅾️\n",
    "\n",
    "For the second test, you are required to check for zero values within the dataset.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check for zero values within the data. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.54980079...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 5.97609561...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here\n",
    "#year = 1963\n",
    "check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "\n",
    "columns_to_check = [col for col in df_1963.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.satisfies(f\"{col}==0\", \"Checking for zeros\", lambda x: x == 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1963).addCheck(check).run()\n",
    "check_result_df_1963_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df_1963_zero.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of non-zero entries for the volume field in 1963 is 99.94%.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to only include entries with non-zero volume\n",
    "df_1963_nonzero = df_1963.filter(col(\"volume\") != 0)\n",
    "\n",
    "# Calculate the percentage of non-zero entries\n",
    "percentage_nonzero = (df_1963_nonzero.count() / df_1963.count()) * 100\n",
    "\n",
    "print(f\"The percentage of non-zero entries for the volume field in 1963 is {percentage_nonzero:.2f}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.52619174...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.07761917...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year = 1974\n",
    "check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "\n",
    "columns_to_check = [col for col in df_1974.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.satisfies(f\"{col}==0\", \"Checking for zeros\", lambda x: x == 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1974).addCheck(check).run()\n",
    "check_result_df_1974_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df_1974_zero.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns contain zeros: ['open', 'volume']\n"
     ]
    }
   ],
   "source": [
    "# year = 1974\n",
    "columns_to_check = [col for col in df_1974.columns if col not in [\"date\", \"stock\"]]\n",
    "cols_with_zeros = []\n",
    "\n",
    "for col in columns_to_check:\n",
    "    if df_1974.filter(f\"{col} == 0\").count() > 0:\n",
    "        cols_with_zeros.append(col)\n",
    "        check = check.satisfies(f\"{col} != 0\", f\"Checking for zeros in {col}\", lambda x: x != 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1974).addCheck(check).run()\n",
    "check_result_df_1974_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "\n",
    "if cols_with_zeros:\n",
    "    print(f\"The following columns contain zeros: {cols_with_zeros}\")\n",
    "else:\n",
    "    print(\"No columns contain zeros.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.51907134...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.06734226...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year = 1985\n",
    "check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "\n",
    "columns_to_check = [col for col in df_1985.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.satisfies(f\"{col}==0\", \"Checking for zeros\", lambda x: x == 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1985).addCheck(check).run()\n",
    "check_result_df_1985_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df_1985_zero.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of zeros in the high column is 0.000%.\n"
     ]
    }
   ],
   "source": [
    "# percentage of zeros are found in the high field in 1985\n",
    "# Count the number of entries in the high column\n",
    "count_high = df_1985.select(\"high\").count()\n",
    "\n",
    "# Count the number of zeros in the high column\n",
    "count_high_zero = df_1985.filter(col(\"high\") == 0).select(\"high\").count()\n",
    "\n",
    "# Calculate the percentage of zero entries\n",
    "percentage_zero = (count_high_zero / count_high) * 100\n",
    "\n",
    "print(f\"The percentage of zeros in the high column is {percentage_zero:.3f}%.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.00169367...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.07827937...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year = 1996\n",
    "check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "\n",
    "columns_to_check = [col for col in df_1996.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.satisfies(f\"{col}==0\", \"Checking for zeros\", lambda x: x == 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1996).addCheck(check).run()\n",
    "check_result_df_1996_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df_1996_zero.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.05299465...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year = 2007\n",
    "check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "\n",
    "columns_to_check = [col for col in df_2007.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.satisfies(f\"{col}==0\", \"Checking for zeros\", lambda x: x == 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_2007).addCheck(check).run()\n",
    "check_result_df_2007_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df_2007_zero.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 7.62682261...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 7.62682261...|\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Integrity Checks|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.02746418...|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year = 2018\n",
    "check = Check(spark, CheckLevel.Warning, \"Integrity Checks\")\n",
    "\n",
    "columns_to_check = [col for col in df_2018.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.satisfies(f\"{col}==0\", \"Checking for zeros\", lambda x: x == 0)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_2018).addCheck(check).run()\n",
    "check_result_df_2018_zero = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df_2018_zero.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Negative values ➖️\n",
    "The third test requires you to check that all values in the data are positive.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to check negative values within the dataset. \n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here\n",
    "#year = 1963\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "columns_to_check = [col for col in df_1963.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.isNonNegative(col)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1963).addCheck(check).run()\n",
    "check_result_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99437477...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1974\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "columns_to_check = [col for col in df_1974.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.isNonNegative(col)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1974).addCheck(check).run()\n",
    "check_result_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns have negative values: ['adj_close']\n"
     ]
    }
   ],
   "source": [
    "# year = 1974\n",
    "# Get the column names that have negative values\n",
    "numeric_cols = [c for c in df_1974.columns if c not in ['date', 'stock'] and df_1974.select(c).dtypes[0][1] in ['bigint', 'double', 'float', 'int']]\n",
    "negative_cols = [c for c in numeric_cols if df_1974.filter(col(c) < 0).count() > 0]\n",
    "\n",
    "if negative_cols:\n",
    "    print(\"The following columns have negative values:\", negative_cols)\n",
    "else:\n",
    "    print(\"No columns have negative values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99515556...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1985\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "columns_to_check = [col for col in df_1985.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.isNonNegative(col)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1985).addCheck(check).run()\n",
    "check_result_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99907791...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1996\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "columns_to_check = [col for col in df_1996.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.isNonNegative(col)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_1996).addCheck(check).run()\n",
    "check_result_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99936366...|\n",
      "|Review Check|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2007\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "columns_to_check = [col for col in df_2007.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.isNonNegative(col)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_2007).addCheck(check).run()\n",
    "check_result_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "|Review Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2018\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "columns_to_check = [col for col in df_2018.columns if col not in [\"date\", \"stock\"]]\n",
    "for col in columns_to_check:\n",
    "    check = check.isNonNegative(col)\n",
    "\n",
    "check_result = VerificationSuite(spark).onData(df_2018).addCheck(check).run()\n",
    "check_result_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 - Determine Maximum Values ⚠️\n",
    "\n",
    "For the fourth test, we want to find the maximum values in the dataset for the numerical fields. Extremum values can often be used to define an upper bound for the column values so we can define them as the threshold values. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Column Profiler Runner` to generate summary statistics for all the available columns. \n",
    ">2. Extract the maximum values for all the numeric columns in the data.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name\t\tMax Value\n",
      "open\t\t303.125\n",
      "high\t\t315.625\n",
      "low\t\t311.875\n",
      "close\t\t313.75\n",
      "adj_close\t\t148.7704620361328\n",
      "volume\t\t20692800.0\n"
     ]
    }
   ],
   "source": [
    "#year = 1963\n",
    "# Get the column names for all numeric columns except \"stock\" and \"date\"\n",
    "num_cols = [c for c in df_1963.columns if c not in ['stock', 'date']]\n",
    "\n",
    "# Compute the maximum value for each numeric column\n",
    "max_values = {}\n",
    "for col_name in num_cols:\n",
    "    max_value = df_1963.select(max(col(col_name))).collect()[0][0]\n",
    "    max_values[col_name] = max_value\n",
    "\n",
    "# Output the table with column name and max values\n",
    "print(\"Column Name\\t\\tMax Value\")\n",
    "for col, value in max_values.items():\n",
    "    print(f\"{col}\\t\\t{value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name\t\tMax Value\n",
      "open\t\t367.5\n",
      "high\t\t376.6666564941406\n",
      "low\t\t364.5833435058594\n",
      "close\t\t367.5\n",
      "adj_close\t\t292.9978332519531\n",
      "volume\t\t75315200.0\n"
     ]
    }
   ],
   "source": [
    "#year = 1974\n",
    "# Get the column names for all numeric columns except \"stock\" and \"date\"\n",
    "num_cols = [c for c in df_1974.columns if c not in ['stock', 'date']]\n",
    "\n",
    "# Compute the maximum value for each numeric column\n",
    "max_values = {}\n",
    "for col_name in num_cols:\n",
    "    max_value = df_1974.select(max(col(col_name))).collect()[0][0]\n",
    "    max_values[col_name] = max_value\n",
    "\n",
    "# Output the table with column name and max values\n",
    "print(\"Column Name\\t\\tMax Value\")\n",
    "for col, value in max_values.items():\n",
    "    print(f\"{col}\\t\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name\t\tMax Value\n",
      "open\t\t100000.0\n",
      "high\t\t224062.5\n",
      "low\t\t219375.0\n",
      "close\t\t221250.0\n",
      "adj_close\t\t190826.34375\n",
      "volume\t\t183495200.0\n"
     ]
    }
   ],
   "source": [
    "#year = 1985\n",
    "# Get the column names for all numeric columns except \"stock\" and \"date\"\n",
    "num_cols = [c for c in df_1985.columns if c not in ['stock', 'date']]\n",
    "\n",
    "# Compute the maximum value for each numeric column\n",
    "max_values = {}\n",
    "for col_name in num_cols:\n",
    "    max_value = df_1985.select(max(col(col_name))).collect()[0][0]\n",
    "    max_values[col_name] = max_value\n",
    "\n",
    "# Output the table with column name and max values\n",
    "print(\"Column Name\\t\\tMax Value\")\n",
    "for col, value in max_values.items():\n",
    "    print(f\"{col}\\t\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name\t\tMax Value\n",
      "open\t\t498750.0\n",
      "high\t\t507500.0\n",
      "low\t\t490000.0\n",
      "close\t\t507500.0\n",
      "adj_close\t\t507500.0\n",
      "volume\t\t546630400.0\n"
     ]
    }
   ],
   "source": [
    "#year = 1996\n",
    "# Get the column names for all numeric columns except \"stock\" and \"date\"\n",
    "num_cols = [c for c in df_1996.columns if c not in ['stock', 'date']]\n",
    "\n",
    "# Compute the maximum value for each numeric column\n",
    "max_values = {}\n",
    "for col_name in num_cols:\n",
    "    max_value = df_1996.select(max(col(col_name))).collect()[0][0]\n",
    "    max_values[col_name] = max_value\n",
    "\n",
    "# Output the table with column name and max values\n",
    "print(\"Column Name\\t\\tMax Value\")\n",
    "for col, value in max_values.items():\n",
    "    print(f\"{col}\\t\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name\t\tMax Value\n",
      "open\t\t62672400384.0\n",
      "high\t\t63503998976.0\n",
      "low\t\t61613998080.0\n",
      "close\t\t62067601408.0\n",
      "adj_close\t\t62067601408.0\n",
      "volume\t\t1835338880.0\n"
     ]
    }
   ],
   "source": [
    "#year = 2007\n",
    "# Get the column names for all numeric columns except \"stock\" and \"date\"\n",
    "num_cols = [c for c in df_2007.columns if c not in ['stock', 'date']]\n",
    "\n",
    "# Compute the maximum value for each numeric column\n",
    "max_values = {}\n",
    "for col_name in num_cols:\n",
    "    max_value = df_2007.select(max(col(col_name))).collect()[0][0]\n",
    "    max_values[col_name] = max_value\n",
    "\n",
    "# Output the table with column name and max values\n",
    "print(\"Column Name\\t\\tMax Value\")\n",
    "for col, value in max_values.items():\n",
    "    print(f\"{col}\\t\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name\t\tMax Value\n",
      "open\t\t117187.5\n",
      "high\t\t125000.0\n",
      "low\t\t109375.0\n",
      "close\t\t109375.0\n",
      "adj_close\t\t109375.0\n",
      "volume\t\t358775712.0\n"
     ]
    }
   ],
   "source": [
    "#year = 2018\n",
    "# Get the column names for all numeric columns except \"stock\" and \"date\"\n",
    "num_cols = [c for c in df_2018.columns if c not in ['stock', 'date']]\n",
    "\n",
    "# Compute the maximum value for each numeric column\n",
    "max_values = {}\n",
    "for col_name in num_cols:\n",
    "    max_value = df_2018.select(max(col(col_name))).collect()[0][0]\n",
    "    max_values[col_name] = max_value\n",
    "\n",
    "# Output the table with column name and max values\n",
    "print(\"Column Name\\t\\tMax Value\")\n",
    "for col, value in max_values.items():\n",
    "    print(f\"{col}\\t\\t{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5 - Stock Tickers 💹️\n",
    "\n",
    "For the fifth test, we want to determine if the stock tickers contained in our dataset are consistent. To do this, you will need to make use of use of the metadata file to check that the stock names used in the dataframe are valid. \n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to determine if the stock tickers contained in the dataset appear in the metadata file.\n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|            check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Consistency Check|    Warning|     Success|ComplianceConstra...|          Success|                  |\n",
      "+-----------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1963\n",
    "unique_stock_symbol = metadata.select(\"Symbol\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Consistency Check\").isContainedIn(\"stock\", unique_stock_symbol)\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df_1963) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|            check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Consistency Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99437477...|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1974\n",
    "unique_stock_symbol = metadata.select(\"Symbol\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Consistency Check\").isContainedIn(\"stock\", unique_stock_symbol)\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df_1974) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|            check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Consistency Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99854320...|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1985\n",
    "unique_stock_symbol = metadata.select(\"Symbol\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Consistency Check\").isContainedIn(\"stock\", unique_stock_symbol)\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df_1985) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|            check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Consistency Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99878647...|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1996\n",
    "unique_stock_symbol = metadata.select(\"Symbol\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Consistency Check\").isContainedIn(\"stock\", unique_stock_symbol)\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df_1996) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|            check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Consistency Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99935598...|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2007\n",
    "unique_stock_symbol = metadata.select(\"Symbol\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Consistency Check\").isContainedIn(\"stock\", unique_stock_symbol)\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df_2007) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|            check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Consistency Check|    Warning|     Warning|ComplianceConstra...|          Failure|Value: 0.99958662...|\n",
      "+-----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2018\n",
    "unique_stock_symbol = metadata.select(\"Symbol\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Consistency Check\").isContainedIn(\"stock\", unique_stock_symbol)\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df_2018) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6 - Duplication 👥️\n",
    "Lastly, we want to determine the uniqueness of the items found in the dataframe. You need to make use of the Verification Suite to check for the validity of the stock tickers. \n",
    "\n",
    "Similar to the previous notebook - `Data_profiling_student_version.ipynb`, the first thing to check will be if the primary key values within the dataset are unique - in our case, that will be a combination of the stock name and the date. Secondly, we want to check if the entries are all unique, which is done by checking for duplicates across that whole dataset.\n",
    "\n",
    "> ℹ️ **Instructions** ℹ️\n",
    ">\n",
    ">1. Make use of the `Verification Suite` and write code to determine the uniqueness of entries contained within the dataset.\n",
    ">2. Display the results of your test.\n",
    ">\n",
    "> *You may use as many cells as necessary*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Primary Key Uniqu...|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Write your code here\n",
    "#year = 1963 uniqueness of primary keys\n",
    "df_1963 = df_1963.withColumn(\"primary_key\", concat(col(\"date\"), col(\"stock\")))\n",
    "\n",
    "# Use Verification Suite to check for primary key uniqueness\n",
    "check1 = Check(spark, CheckLevel.Warning, \"Primary Key Uniqueness Check\")\n",
    "checkResult1 = VerificationSuite(spark) \\\n",
    "    .onData(df_1963) \\\n",
    "    .addCheck(\n",
    "        check1.isUnique(\"primary_key\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Display the results of the test\n",
    "checkResult1_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult1)\n",
    "checkResult1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found:\n",
      "<pydeequ.verification.VerificationResult object at 0x00000164F6A18E80>\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Check for duplicates|    Warning|     Warning|UniquenessConstra...|          Failure|Input data does n...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1963 uniqueness of entries\n",
    "# Create the check for uniqueness across the whole dataset\n",
    "check = Check(spark, CheckLevel.Warning, \"Check for duplicates\").isUnique(\"All columns\")\n",
    "\n",
    "# Run the check\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(df_1963) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "# Print the results\n",
    "if result.status == \"Success\":\n",
    "    print(\"No duplicate entries found\")\n",
    "else:\n",
    "    print(f\"Duplicate entries found:\\n{result}\")\n",
    "    check_result_df = result.checkResultsAsDataFrame(spark, result )\n",
    "    check_result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of uniqueness for all non-key entries in 1963 is: 99.88%\n"
     ]
    }
   ],
   "source": [
    "non_key_cols = ['open', 'low', 'high', 'close', 'adj_close', 'volume']\n",
    "# Count the total number of non-key entries\n",
    "total_entries = df_1963.select([col(c) for c in non_key_cols]).count()\n",
    "\n",
    "# Count the number of unique non-key entries\n",
    "unique_entries = df_1963.select([col(c) for c in non_key_cols]).distinct().count()\n",
    "\n",
    "# Calculate the percentage of uniqueness\n",
    "percentage_uniqueness = unique_entries / total_entries * 100\n",
    "\n",
    "print(f\"The percentage of uniqueness for all non-key entries in 1963 is: {percentage_uniqueness:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Primary Key Uniqu...|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1974 uniqueness of primary keys\n",
    "df_1974 = df_1974.withColumn(\"primary_key\", concat(col(\"date\"), col(\"stock\")))\n",
    "\n",
    "# Use Verification Suite to check for primary key uniqueness\n",
    "check1 = Check(spark, CheckLevel.Warning, \"Primary Key Uniqueness Check\")\n",
    "checkResult1 = VerificationSuite(spark) \\\n",
    "    .onData(df_1974) \\\n",
    "    .addCheck(\n",
    "        check1.isUnique(\"primary_key\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Display the results of the test\n",
    "checkResult1_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult1)\n",
    "checkResult1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found:\n",
      "<pydeequ.verification.VerificationResult object at 0x00000164F65CA280>\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Check for duplicates|    Warning|     Warning|UniquenessConstra...|          Failure|Input data does n...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1974 uniqueness of entries\n",
    "# Create the check for uniqueness across the whole dataset\n",
    "check = Check(spark, CheckLevel.Warning, \"Check for duplicates\").isUnique(\"All columns\")\n",
    "\n",
    "# Run the check\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(df_1974) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "# Print the results\n",
    "if result.status == \"Success\":\n",
    "    print(\"No duplicate entries found\")\n",
    "else:\n",
    "    print(f\"Duplicate entries found:\\n{result}\")\n",
    "    check_result_df = result.checkResultsAsDataFrame(spark, result )\n",
    "    check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Primary Key Uniqu...|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1985 uniqueness of primary keys\n",
    "df_1985 = df_1985.withColumn(\"primary_key\", concat(col(\"date\"), col(\"stock\")))\n",
    "\n",
    "# Use Verification Suite to check for primary key uniqueness\n",
    "check1 = Check(spark, CheckLevel.Warning, \"Primary Key Uniqueness Check\")\n",
    "checkResult1 = VerificationSuite(spark) \\\n",
    "    .onData(df_1985) \\\n",
    "    .addCheck(\n",
    "        check1.isUnique(\"primary_key\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Display the results of the test\n",
    "checkResult1_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult1)\n",
    "checkResult1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found:\n",
      "<pydeequ.verification.VerificationResult object at 0x00000164F67D0880>\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Check for duplicates|    Warning|     Warning|UniquenessConstra...|          Failure|Input data does n...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1985 uniqueness of entries\n",
    "# Create the check for uniqueness across the whole dataset\n",
    "check = Check(spark, CheckLevel.Warning, \"Check for duplicates\").isUnique(\"All columns\")\n",
    "\n",
    "# Run the check\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(df_1985) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "# Print the results\n",
    "if result.status == \"Success\":\n",
    "    print(\"No duplicate entries found\")\n",
    "else:\n",
    "    print(f\"Duplicate entries found:\\n{result}\")\n",
    "    check_result_df = result.checkResultsAsDataFrame(spark, result )\n",
    "    check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Primary Key Uniqu...|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1996 uniqueness of primary keys\n",
    "df_1996 = df_1996.withColumn(\"primary_key\", concat(col(\"date\"), col(\"stock\")))\n",
    "\n",
    "# Use Verification Suite to check for primary key uniqueness\n",
    "check1 = Check(spark, CheckLevel.Warning, \"Primary Key Uniqueness Check\")\n",
    "checkResult1 = VerificationSuite(spark) \\\n",
    "    .onData(df_1996) \\\n",
    "    .addCheck(\n",
    "        check1.isUnique(\"primary_key\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Display the results of the test\n",
    "checkResult1_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult1)\n",
    "checkResult1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found:\n",
      "<pydeequ.verification.VerificationResult object at 0x00000164F65BC9A0>\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Check for duplicates|    Warning|     Warning|UniquenessConstra...|          Failure|Input data does n...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 1996 uniqueness of entries\n",
    "# Create the check for uniqueness across the whole dataset\n",
    "check = Check(spark, CheckLevel.Warning, \"Check for duplicates\").isUnique(\"All columns\")\n",
    "\n",
    "# Run the check\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(df_1996) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "# Print the results\n",
    "if result.status == \"Success\":\n",
    "    print(\"No duplicate entries found\")\n",
    "else:\n",
    "    print(f\"Duplicate entries found:\\n{result}\")\n",
    "    check_result_df = result.checkResultsAsDataFrame(spark, result )\n",
    "    check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Primary Key Uniqu...|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2007 uniqueness of primary keys\n",
    "df_2007 = df_2007.withColumn(\"primary_key\", concat(col(\"date\"), col(\"stock\")))\n",
    "\n",
    "# Use Verification Suite to check for primary key uniqueness\n",
    "check1 = Check(spark, CheckLevel.Warning, \"Primary Key Uniqueness Check\")\n",
    "checkResult1 = VerificationSuite(spark) \\\n",
    "    .onData(df_2007) \\\n",
    "    .addCheck(\n",
    "        check1.isUnique(\"primary_key\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Display the results of the test\n",
    "checkResult1_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult1)\n",
    "checkResult1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found:\n",
      "<pydeequ.verification.VerificationResult object at 0x00000164F65BC1C0>\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Check for duplicates|    Warning|     Warning|UniquenessConstra...|          Failure|Input data does n...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2007 uniqueness of entries\n",
    "# Create the check for uniqueness across the whole dataset\n",
    "check = Check(spark, CheckLevel.Warning, \"Check for duplicates\").isUnique(\"All columns\")\n",
    "\n",
    "# Run the check\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(df_2007) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "# Print the results\n",
    "if result.status == \"Success\":\n",
    "    print(\"No duplicate entries found\")\n",
    "else:\n",
    "    print(f\"Duplicate entries found:\\n{result}\")\n",
    "    check_result_df = result.checkResultsAsDataFrame(spark, result )\n",
    "    check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "|Primary Key Uniqu...|    Warning|     Success|UniquenessConstra...|          Success|                  |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2018 uniqueness of primary keys\n",
    "df_2018 = df_2018.withColumn(\"primary_key\", concat(col(\"date\"), col(\"stock\")))\n",
    "\n",
    "# Use Verification Suite to check for primary key uniqueness\n",
    "check1 = Check(spark, CheckLevel.Warning, \"Primary Key Uniqueness Check\")\n",
    "checkResult1 = VerificationSuite(spark) \\\n",
    "    .onData(df_2018) \\\n",
    "    .addCheck(\n",
    "        check1.isUnique(\"primary_key\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Display the results of the test\n",
    "checkResult1_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult1)\n",
    "checkResult1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found:\n",
      "<pydeequ.verification.VerificationResult object at 0x00000164F66ED250>\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Check for duplicates|    Warning|     Warning|UniquenessConstra...|          Failure|Input data does n...|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#year = 2018 uniqueness of entries\n",
    "\n",
    "# Create the check for uniqueness across the whole dataset\n",
    "check = Check(spark, CheckLevel.Warning, \"Check for duplicates\").isUnique(\"All columns\")\n",
    "\n",
    "# Run the check\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(df_2018) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "# Print the results\n",
    "if result.status == \"Success\":\n",
    "    print(\"No duplicate entries found\")\n",
    "else:\n",
    "    print(f\"Duplicate entries found:\\n{result}\")\n",
    "    check_result_df = result.checkResultsAsDataFrame(spark, result )\n",
    "    check_result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b41f59b882618484a4d28c089dca4efdf4ffb1e043e654ec6730d7439b802f5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
